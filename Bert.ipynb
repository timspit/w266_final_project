{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#set working directory\n",
    "os.chdir(\"/Users/alexdessouky/Desktop/MIDS/w266\")\n",
    "\n",
    "#load training data\n",
    "twitter_train = pd.read_excel('./w266_final_project/StanceDataset/train.xlsx')\n",
    "#train_x = twitter_train[twitter_train['Target'] == 'Hillary Clinton']['Tweet']\n",
    "#train_y = twitter_train[twitter_train['Target'] == 'Hillary Clinton']['Stance']\n",
    "train_x = pd.concat([twitter_train[twitter_train['Stance'] == 'AGAINST'].iloc[0:5,]['Tweet'],\n",
    "                     twitter_train[twitter_train['Stance'] == 'NONE'].iloc[0:5,]['Tweet'],\n",
    "                      twitter_train[twitter_train['Stance'] == 'FAVOR'].iloc[0:5,]['Tweet']],\n",
    "                    axis=0)\n",
    "\n",
    "train_y = np.array([[1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0],\n",
    "                  [0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],\n",
    "                  [0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1]])\n",
    "\n",
    "#load test data\n",
    "twitter_test = pd.read_excel('./w266_final_project/StanceDataset/test.xlsx')\n",
    "#test_x = twitter_test[twitter_test['Target'] == 'Hillary Clinton']['Tweet']\n",
    "#test_y = twitter_test[twitter_test['Target'] == 'Hillary Clinton']['Stance']\n",
    "test_x = pd.concat([twitter_test[twitter_test['Stance'] == 'AGAINST'].iloc[0:5,]['Tweet'],\n",
    "                     twitter_test[twitter_test['Stance'] == 'NONE'].iloc[0:5,]['Tweet'],\n",
    "                      twitter_test[twitter_test['Stance'] == 'FAVOR'].iloc[0:5,]['Tweet']],\n",
    "                    axis=0)\n",
    "test_y = np.array([[1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0],\n",
    "                  [0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],\n",
    "                  [0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the paths to bert & data\n",
    "bert_path =   '/Users/alexdessouky/Desktop/MIDS/w266/bert' \n",
    "data_path = '/Users/alexdessouky/Desktop/MIDS/w266/w266_final_project/StanceDataset'  \n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)\n",
    "sys.path.insert(0,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub\n",
    "\n",
    "# Tensorflow hub path to BERT module of choice\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "# Define maximal length of input 'sentences' (post tokenization).\n",
    "max_length = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1109 10:09:51.304888 140736218923904 deprecation_wrapper.py:119] From /Users/alexdessouky/Desktop/MIDS/w266/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokens surrounded by the [CLS] and [SEP] tokens\n",
    "train_tokens = train_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])\n",
    "test_tokens = test_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])\n",
    "\n",
    "#mask ids (mask out the paddings)\n",
    "train_mask_ids = train_tokens.apply(lambda x: len(x)*[1])\n",
    "test_mask_ids = test_tokens.apply(lambda x: len(x)*[1])\n",
    "\n",
    "train_mask_ids = train_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                      np.array(x)).tolist()\n",
    "test_mask_ids = test_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                    np.array(x)).tolist()\n",
    "\n",
    "#add padding to tokens\n",
    "train_tokens = train_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)\n",
    "test_tokens = test_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)\n",
    "\n",
    "#test/train sequence vectors\n",
    "train_sequenceids = train_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "test_sequenceids = test_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "\n",
    "#convert tokens to sentence ids\n",
    "train_sentenceids = train_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()\n",
    "test_sentenceids = test_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()\n",
    "\n",
    "#bert features\n",
    "bert_train = [np.array(train_sentenceids),np.array(train_mask_ids),np.array(train_sequenceids)]\n",
    "bert_test = [np.array(test_sentenceids),np.array(test_mask_ids),np.array(test_sequenceids)]\n",
    "\n",
    "#labels\n",
    "#stance_labels_train = np.array(twitter_train[twitter_train['Target'] == 'Hillary Clinton']['Stance'].apply(lambda x: \n",
    "#                                                                    2 if x == \"FAVOR\" else \n",
    "#                                                                    (1 if x == \"NONE\" else 0)))\n",
    "\n",
    "#stance_labels_test = np.array(twitter_test[twitter_test['Target'] == 'Hillary Clinton']['Stance'].apply(lambda x: \n",
    "#                                                                    2 if x == \"FAVOR\" else \n",
    "#                                                                    (1 if x == \"NONE\" else 0)))\n",
    "\n",
    "            \n",
    "\n",
    "#PREP LABELS FOR NN\n",
    "#train_y = np.zeros(shape = (stance_labels_train.shape[0],3))\n",
    "#train_y[stance_labels_train == 0,0] = 1\n",
    "#train_y[stance_labels_train == 1,1] = 1\n",
    "#train_y[stance_labels_train == 2,2] = 1\n",
    "\n",
    "#test_y = np.zeros(shape = (stance_labels_test.shape[0],3))\n",
    "#test_y[stance_labels_test == 0,0] = 1\n",
    "#test_y[stance_labels_test == 1,1] = 1\n",
    "#test_y[stance_labels_test == 2,2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,   137, 21359,  1181,  1665,  5082,  1584,  1262,   117,\n",
       "         108,  9918,  2346,  4121,  1942,  4638,  1708, 22552,  1197,\n",
       "        1131,  7960,  4044,   116,  1476,  1377, 18931, 24853,   117,\n",
       "        7155,  4167, 21091,  5796,  1104,  4019,   120,  2887,  1231,\n",
       "         108,  3096,  5084, 19888,   117,  3576,   108,   189, 18982,\n",
       "         102,     0,     0,     0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentenceids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layer to create Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_url,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        \n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "        \n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        \n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        \n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model(max_input_length, train_layers, optimizer = tf.keras.optimizers.Adam(learning_rate=1)):\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers)(bert_inputs)\n",
    "    \n",
    "    \n",
    "    #dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dropout = tf.keras.layers.Dropout(rate=0.3)(bert_sequence)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(3, activation='softmax', name='classification')(dropout)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['categorical_accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_5 (BertLayer)        (None, 768)          108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 768)          0           bert_layer_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 3)            2307        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 108,933,703\n",
      "Trainable params: 6,498,051\n",
      "Non-trainable params: 102,435,652\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "15/15 [==============================] - 3s 211ms/sample - loss: 1.2948 - categorical_accuracy: 0.4000\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 1s 99ms/sample - loss: 1.8935 - categorical_accuracy: 0.3333\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 1s 99ms/sample - loss: 0.9469 - categorical_accuracy: 0.5333\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 1s 99ms/sample - loss: 0.9295 - categorical_accuracy: 0.5333\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 1s 99ms/sample - loss: 0.6980 - categorical_accuracy: 0.7333\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 2s 102ms/sample - loss: 0.2387 - categorical_accuracy: 0.9333\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 2s 103ms/sample - loss: 0.1473 - categorical_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 2s 103ms/sample - loss: 0.1356 - categorical_accuracy: 0.9333\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 2s 104ms/sample - loss: 0.0305 - categorical_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 2s 113ms/sample - loss: 0.0104 - categorical_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 2s 105ms/sample - loss: 0.0036 - categorical_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 2s 103ms/sample - loss: 0.0027 - categorical_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 2s 107ms/sample - loss: 6.8626e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 2s 105ms/sample - loss: 8.2822e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 2s 103ms/sample - loss: 7.3789e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 2s 106ms/sample - loss: 4.0777e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 2s 105ms/sample - loss: 3.1090e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 2s 104ms/sample - loss: 7.7450e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 2s 113ms/sample - loss: 2.5651e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 2s 110ms/sample - loss: 1.5078e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 2s 106ms/sample - loss: 1.4486e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 2s 106ms/sample - loss: 2.8827e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 2s 108ms/sample - loss: 2.5272e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 2s 132ms/sample - loss: 8.1786e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 2s 116ms/sample - loss: 1.6446e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 2s 130ms/sample - loss: 1.3551e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 2s 113ms/sample - loss: 5.1789e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 2s 133ms/sample - loss: 7.6880e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 2s 114ms/sample - loss: 4.4885e-05 - categorical_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 2s 117ms/sample - loss: 4.1865e-05 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a7730a400>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start session\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model(max_length + 1, train_layers=12, optimizer = 'adam')\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    bert_train, \n",
    "    train_y,\n",
    "    #validation_data=[bert_test, test_y],\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    batch_size=32)#,\n",
    "    #callbacks=[tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(bert_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_plot(confusion_matrix, target_names):\n",
    "    # Plot confusion matrix (via imshow)\n",
    "    plt.imshow(confusion_matrix, interpolation = \"nearest\", cmap = plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Loop through each value of the matrix to add data labels\n",
    "    width, height = confusion_matrix.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(confusion_matrix[x][y]), xy = (y, x), \n",
    "                        horizontalalignment = \"center\",\n",
    "                        verticalalignment = \"center\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    \n",
    "def metrics(true_labels, test_probs):\n",
    "    \n",
    "    #find predicted labels\n",
    "    test_predicts = np.argmax(test_probs, axis = 1)\n",
    "    \n",
    "    #calculate f1 score\n",
    "    f1 = f1_score(true_labels, test_predicts, average = 'macro')\n",
    "    \n",
    "    print(\"F1 macro score:\", f1)\n",
    "    \n",
    "    print(classification_report(y_true = true_labels, \n",
    "                                        y_pred = test_predicts,\n",
    "                                        target_names = ['Against', 'None', 'Favor']))\n",
    "    \n",
    "    confuse = confusion_matrix(y_true = true_labels, y_pred = test_predicts)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    confusion_plot(confuse, ['Against', 'None', 'Favor'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
