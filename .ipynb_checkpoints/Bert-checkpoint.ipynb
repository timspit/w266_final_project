{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#set working directory\n",
    "os.chdir(\"/Users/alexdessouky/Desktop/MIDS/w266\")\n",
    "\n",
    "#load training data\n",
    "twitter_train = pd.read_excel('./w266_final_project/StanceDataset/train.xlsx')\n",
    "#train_x = twitter_train[twitter_train['Target'] == 'Hillary Clinton']['Tweet']\n",
    "#train_y = twitter_train[twitter_train['Target'] == 'Hillary Clinton']['Stance']\n",
    "train_x = pd.concat([twitter_train[twitter_train['Stance'] == 'AGAINST'].iloc[0:5,]['Tweet'],\n",
    "                     twitter_train[twitter_train['Stance'] == 'NONE'].iloc[0:5,]['Tweet'],\n",
    "                      twitter_train[twitter_train['Stance'] == 'FAVOR'].iloc[0:5,]['Tweet']],\n",
    "                    axis=0)\n",
    "\n",
    "train_y = np.array([[1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0],\n",
    "                  [0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],\n",
    "                  [0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1]])\n",
    "\n",
    "#load test data\n",
    "twitter_test = pd.read_excel('./w266_final_project/StanceDataset/test.xlsx')\n",
    "#test_x = twitter_test[twitter_test['Target'] == 'Hillary Clinton']['Tweet']\n",
    "#test_y = twitter_test[twitter_test['Target'] == 'Hillary Clinton']['Stance']\n",
    "test_x = pd.concat([twitter_test[twitter_test['Stance'] == 'AGAINST'].iloc[0:5,]['Tweet'],\n",
    "                     twitter_test[twitter_test['Stance'] == 'NONE'].iloc[0:5,]['Tweet'],\n",
    "                      twitter_test[twitter_test['Stance'] == 'FAVOR'].iloc[0:5,]['Tweet']],\n",
    "                    axis=0)\n",
    "test_y = np.array([[1,0,0],[1,0,0],[1,0,0],[1,0,0],[1,0,0],\n",
    "                  [0,1,0],[0,1,0],[0,1,0],[0,1,0],[0,1,0],\n",
    "                  [0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the paths to bert & data\n",
    "local_bert_path =   '/Users/alexdessouky/Desktop/MIDS/w266/bert' \n",
    "data_path = '/Users/alexdessouky/Desktop/MIDS/w266/w266_final_project/StanceDataset'  \n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)\n",
    "sys.path.insert(0,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub\n",
    "\n",
    "# Tensorflow hub path to BERT module of choice\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "# Define maximal length of input 'sentences' (post tokenization).\n",
    "max_length = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_x.apply(lambda x: len(tokenizer.tokenize(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokens surrounded by the [CLS] and [SEP] tokens\n",
    "train_tokens = train_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])\n",
    "test_tokens = test_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])\n",
    "\n",
    "#mask ids (mask out the paddings)\n",
    "train_mask_ids = train_tokens.apply(lambda x: len(x)*[1])\n",
    "test_mask_ids = test_tokens.apply(lambda x: len(x)*[1])\n",
    "\n",
    "train_mask_ids = train_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                      np.array(x)).tolist()\n",
    "test_mask_ids = test_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                    np.array(x)).tolist()\n",
    "\n",
    "#add padding to tokens\n",
    "train_tokens = train_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)\n",
    "test_tokens = test_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)\n",
    "\n",
    "#test/train sequence vectors\n",
    "train_sequenceids = train_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "test_sequenceids = test_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "\n",
    "#convert tokens to sentence ids\n",
    "train_sentenceids = train_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()\n",
    "test_sentenceids = test_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()\n",
    "\n",
    "#bert features\n",
    "bert_train = [np.array(train_sentenceids),np.array(train_mask_ids),np.array(train_sequenceids)]\n",
    "bert_test = [np.array(test_sentenceids),np.array(test_mask_ids),np.array(test_sequenceids)]\n",
    "\n",
    "#labels\n",
    "#stance_labels_train = np.array(twitter_train[twitter_train['Target'] == 'Hillary Clinton']['Stance'].apply(lambda x: \n",
    "#                                                                    2 if x == \"FAVOR\" else \n",
    "#                                                                    (1 if x == \"NONE\" else 0)))\n",
    "\n",
    "#stance_labels_test = np.array(twitter_test[twitter_test['Target'] == 'Hillary Clinton']['Stance'].apply(lambda x: \n",
    "#                                                                    2 if x == \"FAVOR\" else \n",
    "#                                                                    (1 if x == \"NONE\" else 0)))\n",
    "\n",
    "            \n",
    "\n",
    "#PREP LABELS FOR NN\n",
    "#train_y = np.zeros(shape = (stance_labels_train.shape[0],3))\n",
    "#train_y[stance_labels_train == 0,0] = 1\n",
    "#train_y[stance_labels_train == 1,1] = 1\n",
    "#train_y[stance_labels_train == 2,2] = 1\n",
    "\n",
    "#test_y = np.zeros(shape = (stance_labels_test.shape[0],3))\n",
    "#test_y[stance_labels_test == 0,0] = 1\n",
    "#test_y[stance_labels_test == 1,1] = 1\n",
    "#test_y[stance_labels_test == 2,2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layer to create Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model(max_input_length, train_layers, optimizer = tf.keras.optimizers.Adam(learning_rate=1)):\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers)(bert_inputs)\n",
    "    \n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.3)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(3, activation='softmax', name='classification')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['categorical_accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_7 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 3)            771         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 110,302,525\n",
      "Trainable params: 85,842,691\n",
      "Non-trainable params: 24,459,834\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "15/15 [==============================] - 12s 776ms/sample - loss: 1.4850 - categorical_accuracy: 0.4000\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 4s 246ms/sample - loss: 1.8658 - categorical_accuracy: 0.4000\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 4s 251ms/sample - loss: 2.4596 - categorical_accuracy: 0.3333\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 3s 232ms/sample - loss: 1.4601 - categorical_accuracy: 0.2667\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 4s 236ms/sample - loss: 2.7947 - categorical_accuracy: 0.3333\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 4s 244ms/sample - loss: 1.9157 - categorical_accuracy: 0.5333\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 4s 264ms/sample - loss: 1.3363 - categorical_accuracy: 0.3333\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 4s 252ms/sample - loss: 1.8117 - categorical_accuracy: 0.3333\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 4s 249ms/sample - loss: 2.7811 - categorical_accuracy: 0.3333\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 4s 238ms/sample - loss: 1.4421 - categorical_accuracy: 0.3333\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 4s 258ms/sample - loss: 1.2946 - categorical_accuracy: 0.2667\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 4s 236ms/sample - loss: 1.5416 - categorical_accuracy: 0.2667\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 4s 240ms/sample - loss: 1.3861 - categorical_accuracy: 0.1333\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 4s 237ms/sample - loss: 1.3305 - categorical_accuracy: 0.2000\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 4s 235ms/sample - loss: 1.0949 - categorical_accuracy: 0.3333\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 4s 237ms/sample - loss: 1.0823 - categorical_accuracy: 0.4667\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 4s 250ms/sample - loss: 1.1013 - categorical_accuracy: 0.4000\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 4s 237ms/sample - loss: 1.1050 - categorical_accuracy: 0.4000\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 3s 233ms/sample - loss: 1.0854 - categorical_accuracy: 0.3333\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 4s 235ms/sample - loss: 1.2225 - categorical_accuracy: 0.2000\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 4s 266ms/sample - loss: 1.1982 - categorical_accuracy: 0.2667\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 4s 247ms/sample - loss: 1.3551 - categorical_accuracy: 0.0667\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 4s 239ms/sample - loss: 1.2506 - categorical_accuracy: 0.2000\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 4s 234ms/sample - loss: 1.1980 - categorical_accuracy: 0.3333\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 3s 230ms/sample - loss: 1.0410 - categorical_accuracy: 0.4667\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 4s 239ms/sample - loss: 1.2450 - categorical_accuracy: 0.2667\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 4s 239ms/sample - loss: 1.1159 - categorical_accuracy: 0.4000\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 4s 239ms/sample - loss: 1.2513 - categorical_accuracy: 0.2000\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 4s 235ms/sample - loss: 1.2697 - categorical_accuracy: 0.4000\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 3s 229ms/sample - loss: 1.0780 - categorical_accuracy: 0.5333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d8b5ce6d8>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start session\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model(max_length + 1, train_layers=12, optimizer = 'adam')\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    bert_train, \n",
    "    train_y,\n",
    "    #validation_data=[bert_test, test_y],\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    batch_size=32)#,\n",
    "    #callbacks=[tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_plot(confusion_matrix, target_names):\n",
    "    # Plot confusion matrix (via imshow)\n",
    "    plt.imshow(confusion_matrix, interpolation = \"nearest\", cmap = plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Loop through each value of the matrix to add data labels\n",
    "    width, height = confusion_matrix.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(confusion_matrix[x][y]), xy = (y, x), \n",
    "                        horizontalalignment = \"center\",\n",
    "                        verticalalignment = \"center\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    \n",
    "def metrics(true_labels, test_probs):\n",
    "    \n",
    "    #find predicted labels\n",
    "    test_predicts = np.argmax(test_probs, axis = 1)\n",
    "    \n",
    "    #calculate f1 score\n",
    "    f1 = f1_score(true_labels, test_predicts, average = 'macro')\n",
    "    \n",
    "    print(\"F1 macro score:\", f1)\n",
    "    \n",
    "    print(classification_report(y_true = true_labels, \n",
    "                                        y_pred = test_predicts,\n",
    "                                        target_names = ['Against', 'None', 'Favor']))\n",
    "    \n",
    "    confuse = confusion_matrix(y_true = true_labels, y_pred = test_predicts)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    confusion_plot(confuse, ['Against', 'None', 'Favor'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
