{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#set working directory\n",
    "# os.chdir(\"/Users/alexdessouky/Desktop/MIDS/w266\")\n",
    "\n",
    "#load training data\n",
    "twitter_train = pd.read_excel('./StanceDataset/train.xlsx')\n",
    "train_x = twitter_train['Tweet']\n",
    "train_y = twitter_train['Stance']\n",
    "\n",
    "\n",
    "#load test data\n",
    "twitter_test = pd.read_excel('./StanceDataset/test.xlsx')\n",
    "test_x = twitter_test['Tweet']\n",
    "test_y = twitter_test[['Stance','Target']]\n",
    "\n",
    "#carve out 15% of test data to use as dev data\n",
    "test_x, dev_x, test_y, dev_y = train_test_split(test_x, test_y, test_size=0.15, random_state=42)\n",
    "\n",
    "#need to save the target arrays for test & dev separately; then can drop them from our y arrays\n",
    "target_test = test_y['Target']\n",
    "target_dev = dev_y['Target']\n",
    "\n",
    "test_y = test_y['Stance']\n",
    "dev_y = dev_y['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the paths to bert & data\n",
    "# bert_path =   '/Users/alexdessouky/Desktop/MIDS/w266/bert' \n",
    "#data_path = '/Users/alexdessouky/Desktop/MIDS/w266/w266_final_project/StanceDataset'  \n",
    "bert_path = r'/home/timspittle/w266_final_project/bert/' \n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,bert_path)\n",
    "# sys.path.insert(0,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/timspittle/w266_final_project/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub\n",
    "\n",
    "# Tensorflow hub path to BERT module of choice\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "# Define maximal length of input 'sentences' (post tokenization).\n",
    "max_length = 83 ## max length with no preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/timspittle/w266_final_project/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/timspittle/w266_final_project/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we want to \"Clean\" the tweets; uncomment out this cell (performs better without)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def preprocess_tweets(x):\n",
    "    \n",
    "    # Remove punctuation EXCEPT for hashtags (#) and handles (@)\n",
    "#    exclude_punc = [punc for punc in string.punctuation if punc not in ['#', '@']]\n",
    "#    x_nopunc = ''.join(ch for ch in x if ch not in exclude_punc)\n",
    "\n",
    "    # lower case\n",
    "#    x_lower = x_nopunc.lower()\n",
    "    \n",
    "    # Replace digits with DIGIT\n",
    "#    x_digits = re.sub(\"\\d+\", \"DIGIT\", x_lower)\n",
    "    \n",
    "#    return x_digits\n",
    "\n",
    "#train_x = twitter_train['Tweet'].apply(lambda x: preprocess_tweets(x))\n",
    "#test_x = twitter_test['Tweet'].apply(lambda x: preprocess_tweets(x))\n",
    "\n",
    "#max_length = 95 #redefine max length - this is what it is when we clean the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tokens surrounded by the [CLS] and [SEP] tokens\n",
    "train_tokens = train_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])\n",
    "test_tokens = test_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])\n",
    "dev_tokens = dev_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])\n",
    "\n",
    "#mask ids (mask out the paddings)\n",
    "train_mask_ids = train_tokens.apply(lambda x: len(x)*[1])\n",
    "test_mask_ids = test_tokens.apply(lambda x: len(x)*[1])\n",
    "dev_mask_ids = dev_tokens.apply(lambda x: len(x)*[1])\n",
    "\n",
    "train_mask_ids = train_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                      np.array(x)).tolist()\n",
    "test_mask_ids = test_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                    np.array(x)).tolist()\n",
    "dev_mask_ids = dev_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                    np.array(x)).tolist()\n",
    "\n",
    "#add padding to tokens\n",
    "train_tokens = train_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)\n",
    "test_tokens = test_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)\n",
    "dev_tokens = dev_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)\n",
    "\n",
    "#test/train sequence vectors\n",
    "train_sequenceids = train_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "test_sequenceids = test_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "dev_sequenceids = dev_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "\n",
    "#convert tokens to sentence ids\n",
    "train_sentenceids = train_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()\n",
    "test_sentenceids = test_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()\n",
    "dev_sentenceids = dev_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()\n",
    "\n",
    "#bert features\n",
    "bert_train = [np.array(train_sentenceids),np.array(train_mask_ids),np.array(train_sequenceids)]\n",
    "bert_test = [np.array(test_sentenceids),np.array(test_mask_ids),np.array(test_sequenceids)]\n",
    "bert_dev = [np.array(dev_sentenceids),np.array(dev_mask_ids),np.array(dev_sequenceids)]\n",
    "\n",
    "#labels\n",
    "stance_labels_train = np.array(twitter_train['Stance'].apply(lambda x: \n",
    "                                                                    2 if x == \"FAVOR\" else \n",
    "                                                                    (1 if x == \"NONE\" else 0)))\n",
    "\n",
    "stance_labels_test = np.array(pd.Series(test_y).apply(lambda x: \n",
    "                                                                2 if x == \"FAVOR\" else \n",
    "                                                                (1 if x == \"NONE\" else 0)))\n",
    "\n",
    "stance_labels_dev = np.array(pd.Series(dev_y).apply(lambda x: \n",
    "                                                                2 if x == \"FAVOR\" else \n",
    "                                                                (1 if x == \"NONE\" else 0)))\n",
    "      \n",
    "\n",
    "#PREP LABELS FOR NN\n",
    "train_y = np.zeros(shape = (stance_labels_train.shape[0],3))\n",
    "train_y[stance_labels_train == 0,0] = 1\n",
    "train_y[stance_labels_train == 1,1] = 1\n",
    "train_y[stance_labels_train == 2,2] = 1\n",
    "\n",
    "test_y = np.zeros(shape = (stance_labels_test.shape[0],3))\n",
    "test_y[stance_labels_test == 0,0] = 1\n",
    "test_y[stance_labels_test == 1,1] = 1\n",
    "test_y[stance_labels_test == 2,2] = 1\n",
    "\n",
    "dev_y = np.zeros(shape = (stance_labels_dev.shape[0],3))\n",
    "dev_y[stance_labels_dev == 0,0] = 1\n",
    "dev_y[stance_labels_dev == 1,1] = 1\n",
    "dev_y[stance_labels_dev == 2,2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2914, 83)(2914, 83)(2914, 83)\n",
      "(2914, 3)\n",
      "(1662, 83)(1662, 83)(1662, 83)\n",
      "(1662, 3)\n",
      "(294, 83)(294, 83)(294, 83)\n",
      "(294, 3)\n"
     ]
    }
   ],
   "source": [
    "print(str(bert_train[0].shape) + str(bert_train[1].shape) + str(bert_train[2].shape))\n",
    "print(train_y.shape)\n",
    "\n",
    "print(str(bert_test[0].shape) + str(bert_test[1].shape) + str(bert_test[2].shape))\n",
    "print(test_y.shape)\n",
    "\n",
    "print(str(bert_dev[0].shape) + str(bert_dev[1].shape) + str(bert_dev[2].shape))\n",
    "print(dev_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to obtain a subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_subset(topic_list):\n",
    "        \n",
    "    #convert topic list to lower case\n",
    "    for i in range(0,len(topic_list)):\n",
    "        topic_list[i] = topic_list[i].lower()\n",
    "    \n",
    "    #find boolean series of all rows pertaining to the relevant topics\n",
    "    matches_train = twitter_train['Target'].apply(lambda x: x.lower() in topic_list)\n",
    "    matches_test = pd.Series(target_test).apply(lambda x: x.lower() in topic_list)\n",
    "    matches_dev = pd.Series(target_dev).apply(lambda x: x.lower() in topic_list)\n",
    "    \n",
    "    #subset labels\n",
    "    train_y_sub = train_y[matches_train,:]\n",
    "    test_y_sub = test_y[matches_test,:]\n",
    "    dev_y_sub = dev_y[matches_dev,:]\n",
    "    \n",
    "    \n",
    "    #initialize bert training lists for subsetting\n",
    "    bert_train_sub = []\n",
    "    bert_test_sub = []\n",
    "    bert_dev_sub = []\n",
    "    \n",
    "    for train_input in bert_train:\n",
    "        bert_train_sub.append(train_input[matches_train,:])\n",
    "        \n",
    "    for test_input in bert_test:\n",
    "        bert_test_sub.append(test_input[matches_test,:])\n",
    "        \n",
    "    for dev_input in bert_dev:\n",
    "        bert_dev_sub.append(dev_input[matches_dev,:])\n",
    "        \n",
    "    #also need to return the test labels in this format for metrics\n",
    "    true_labels = stance_labels_test[matches_test]\n",
    "    \n",
    "    return bert_train_sub, train_y_sub, bert_test_sub, test_y_sub, true_labels, bert_dev_sub, dev_y_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Run Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_plot(confusion_matrix, target_names):\n",
    "    # Plot confusion matrix (via imshow)\n",
    "    plt.imshow(confusion_matrix, interpolation = \"nearest\", cmap = plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Loop through each value of the matrix to add data labels\n",
    "    width, height = confusion_matrix.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(confusion_matrix[x][y]), xy = (y, x), \n",
    "                        horizontalalignment = \"center\",\n",
    "                        verticalalignment = \"center\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    \n",
    "def metrics(true_labels, test_probs):\n",
    "    \n",
    "    #find predicted labels\n",
    "    test_predicts = np.argmax(test_probs, axis = 1)\n",
    "    \n",
    "    #calculate f1 score and print classification report\n",
    "    class_report = classification_report(y_true = true_labels, \n",
    "                                        y_pred = test_predicts,\n",
    "                                        target_names = ['Against', 'None', 'Favor'], output_dict=True)\n",
    "    \n",
    "    print(\"F1 macro score (Favor and Against only)\", str((class_report['Favor']['f1-score'] + \n",
    "         class_report['Against']['f1-score']) / 2))\n",
    "    \n",
    "    print(classification_report(y_true = true_labels, \n",
    "                                        y_pred = test_predicts,\n",
    "                                        target_names = ['Against', 'None', 'Favor']))\n",
    "    \n",
    "    confuse = confusion_matrix(y_true = true_labels, y_pred = test_predicts)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    confusion_plot(confuse, ['Against', 'None', 'Favor'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layer to create Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer_Seq(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer_Seq, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_url,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name and not \"/pooler/\" in var.name]\n",
    "        \n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "        \n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        \n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        \n",
    "        super(BertLayer_Seq, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"sequence_output\"\n",
    "        ]\n",
    "        \n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model_seq(max_length, train_layers, optimizer, dropout_rate):\n",
    "    \n",
    "    #inputs\n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    #obtain bert Sequential output\n",
    "    bert_sequence = BertLayer_Seq(n_fine_tune_layers=train_layers)(bert_inputs)\n",
    "    \n",
    "    #perform dropout\n",
    "    dropout1= tf.keras.layers.Dropout(rate=dropout_rate)(bert_sequence)\n",
    "    \n",
    "    #LSTM layer to handle Sequential output\n",
    "    lstm1 = tf.keras.layers.LSTM(128, name='lstm1')(dropout1)\n",
    "    \n",
    "    #Feed LSTM layer to 1 dense layer\n",
    "    dense1 = tf.keras.layers.Dense(64, activation='relu', \n",
    "                                 kernel_initializer = tf.keras.initializers.he_normal(), \n",
    "                                 name='dense1')(lstm1)\n",
    "    \n",
    "    #perform another dropout\n",
    "    dropout2 = tf.keras.layers.Dropout(rate=dropout_rate)(dense1)\n",
    "    \n",
    "    #classification layer\n",
    "    pred = tf.keras.layers.Dense(3, activation='softmax', name='classification')(dropout2)\n",
    "    \n",
    "    #compile model\n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['categorical_accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atheism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, true_labels, val_data, val_y = topic_subset(['Atheism'])\n",
    "\n",
    "#Start session\n",
    "compute_class = compute_class_weight('balanced', np.unique(true_labels), true_labels)\n",
    "weights = {0: compute_class[0], 1:compute_class[1], 2:compute_class[2]}\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model_seq(max_length = max_length, \n",
    "                     train_layers=6, \n",
    "                     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1 = 0.9,\n",
    "                                                         beta_2 = 0.999),\n",
    "                     dropout_rate= 0.5)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    validation_data = [val_data, val_y],\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    class_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_data)\n",
    "metrics(true_labels, test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hillary Clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, true_labels, val_data, val_y = topic_subset(['Hillary Clinton'])\n",
    "\n",
    "#Start session\n",
    "compute_class = compute_class_weight('balanced', np.unique(true_labels), true_labels)\n",
    "weights = {0: compute_class[0], 1:compute_class[1], 2:compute_class[2]}\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model_seq(max_length = max_length, \n",
    "                     train_layers=6, \n",
    "                     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1 = 0.9,\n",
    "                                                         beta_2 = 0.999),\n",
    "                     dropout_rate= 0.5)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    validation_data = [val_data, val_y],\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    class_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_data)\n",
    "metrics(true_labels, test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, true_labels, val_data, val_y = topic_subset(['Legalization of Abortion'])\n",
    "\n",
    "#Start session\n",
    "compute_class = compute_class_weight('balanced', np.unique(true_labels), true_labels)\n",
    "weights = {0: compute_class[0], 1:compute_class[1], 2:compute_class[2]}\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model_seq(max_length = max_length, \n",
    "                     train_layers=6, \n",
    "                     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1 = 0.9,\n",
    "                                                         beta_2 = 0.999),\n",
    "                     dropout_rate= 0.5)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    validation_data = [val_data, val_y],\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    class_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_data)\n",
    "metrics(true_labels, test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, true_labels, val_data, val_y = topic_subset(['Climate Change is a Real Concern'])\n",
    "\n",
    "#Start session\n",
    "compute_class = compute_class_weight('balanced', np.unique(true_labels), true_labels)\n",
    "weights = {0: compute_class[0], 1:compute_class[1], 2:compute_class[2]}\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model_seq(max_length = max_length, \n",
    "                     train_layers=6, \n",
    "                     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1 = 0.9,\n",
    "                                                         beta_2 = 0.999),\n",
    "                     dropout_rate= 0.5)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    validation_data = [val_data, val_y],\n",
    "    epochs=7,\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    class_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_data)\n",
    "metrics(true_labels, test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feminism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, true_labels, val_data, val_y = topic_subset(['Feminist Movement'])\n",
    "\n",
    "#Start session\n",
    "compute_class = compute_class_weight('balanced', np.unique(true_labels), true_labels)\n",
    "weights = {0: compute_class[0], 1:compute_class[1], 2:compute_class[2]}\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model_seq(max_length = max_length, \n",
    "                     train_layers=6, \n",
    "                     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1 = 0.9,\n",
    "                                                         beta_2 = 0.999),\n",
    "                     dropout_rate= 0.5)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    validation_data = [val_data, val_y],\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    class_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_data)\n",
    "metrics(true_labels, test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels, true_labels = topic_subset(['Hillary Clinton', \n",
    "                                                                              'Legalization of Abortion', \n",
    "                                                                              'Climate Change is a Real Concern',\n",
    "                                                                             'Atheism',\n",
    "                                                                             'Feminist Movement'])\n",
    "\n",
    "\n",
    "#Start session\n",
    "compute_class = compute_class_weight('balanced', np.unique(true_labels), true_labels)\n",
    "weights = {0: compute_class[0], 1:compute_class[1], 2:compute_class[2]}\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model_2(max_length, train_layers=12, optimizer = 'adam')\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    validation_data=[test_data, test_labels],\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    class_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_data)\n",
    "metrics(true_labels, test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_true > 1) | (y_true < 1)\n",
    "    \n",
    "    mask = tf.reshape(mask, [-1])\n",
    "    \n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  \n",
    "    \n",
    "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, 3])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask)\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loss function\n",
    "# y_true = tf.constant([[1],[0],[2]])\n",
    "\n",
    "# y_pred = tf.constant([\n",
    "#     [0.04,0.96,0],\n",
    "#     [0.7,0.3,0],\n",
    "#     [0,0.45,0.55]\n",
    "# ])\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# custom_loss(y_true, y_pred).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc(y_true, y_pred):\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_true > 1) | (y_true < 1)\n",
    "    \n",
    "    mask = tf.reshape(mask, [-1])\n",
    "    \n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, 3]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_subset_custom(topic_list):\n",
    "        \n",
    "    #convert topic list to lower case\n",
    "    for i in range(0,len(topic_list)):\n",
    "        topic_list[i] = topic_list[i].lower()\n",
    "    \n",
    "    #find boolean series of all rows pertaining to the relevant topics\n",
    "    matches_train = twitter_train['Target'].apply(lambda x: x.lower() in topic_list)\n",
    "    matches_test = pd.Series(target_test).apply(lambda x: x.lower() in topic_list)\n",
    "    matches_dev = pd.Series(target_dev).apply(lambda x: x.lower() in topic_list)\n",
    "    \n",
    "    #subset labels\n",
    "    train_y_sub = stance_labels_train[matches_train]\n",
    "    test_y_sub = stance_labels_test[matches_test]\n",
    "    dev_y_sub = stance_labels_dev[matches_dev]\n",
    "    #train_y_sub = np.reshape(stance_labels_train[matches_train], (stance_labels_train[matches_train].shape[0],1))\n",
    "    #test_y_sub = np.reshape(stance_labels_test[matches_test], (stance_labels_test[matches_test].shape[0],1))\n",
    "    #dev_y_sub = np.reshape(stance_labels_dev[matches_dev], (stance_labels_dev[matches_dev].shape[0],1))\n",
    "    \n",
    "    \n",
    "    #initialize bert training lists for subsetting\n",
    "    bert_train_sub = []\n",
    "    bert_test_sub = []\n",
    "    bert_dev_sub = []\n",
    "    \n",
    "    for train_input in bert_train:\n",
    "        bert_train_sub.append(train_input[matches_train,:])\n",
    "        \n",
    "    for test_input in bert_test:\n",
    "        bert_test_sub.append(test_input[matches_test,:])\n",
    "        \n",
    "    for dev_input in bert_dev:\n",
    "        bert_dev_sub.append(dev_input[matches_dev,:])\n",
    "        \n",
    "    #also need to return the test labels in this format for metrics\n",
    "    true_labels = stance_labels_test[matches_test]\n",
    "    \n",
    "    return bert_train_sub, train_y_sub, bert_test_sub, test_y_sub, true_labels, bert_dev_sub, dev_y_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_model_seq_custom(max_length, train_layers, optimizer, dropout_rate):\n",
    "    \n",
    "    #inputs\n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    #obtain bert Sequential output\n",
    "    bert_sequence = BertLayer_Seq(n_fine_tune_layers=train_layers)(bert_inputs)\n",
    "    \n",
    "    #perform dropout\n",
    "    dropout1= tf.keras.layers.Dropout(rate=dropout_rate)(bert_sequence)\n",
    "    \n",
    "    #LSTM layer to handle Sequential output\n",
    "    lstm1 = tf.keras.layers.LSTM(128, name='lstm1')(dropout1)\n",
    "    \n",
    "    #Feed LSTM layer to 1 dense layer\n",
    "    dense1 = tf.keras.layers.Dense(64, activation='relu', \n",
    "                                 kernel_initializer = tf.keras.initializers.he_normal(), \n",
    "                                 name='dense1')(lstm1)\n",
    "    \n",
    "    #perform another dropout\n",
    "    dropout2 = tf.keras.layers.Dropout(rate=dropout_rate)(dense1)\n",
    "    \n",
    "    #classification layer\n",
    "    pred = tf.keras.layers.Dense(3, activation='softmax', name='classification')(dropout2)\n",
    "    \n",
    "    #compile model\n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss=custom_loss, optimizer=optimizer, metrics = [custom_acc])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/timspittle/.local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/timspittle/.local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/timspittle/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/timspittle/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 83)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 83)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 83)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer__seq (BertLayer_Seq) (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 768)    0           bert_layer__seq[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 128)          459264      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 64)           8256        lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 3)            195         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,399,111\n",
      "Trainable params: 5,191,683\n",
      "Non-trainable params: 104,207,428\n",
      "__________________________________________________________________________________________________\n",
      "Train on 513 samples, validate on 35 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Can not squeeze dim[0], expected a dimension of 1, got 32\n\t [[{{node loss_1/classification_loss/weighted_loss/Squeeze}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4425ca9d4c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     class_weight = weights)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Can not squeeze dim[0], expected a dimension of 1, got 32\n\t [[{{node loss_1/classification_loss/weighted_loss/Squeeze}}]]"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels, true_labels, val_data, val_y = topic_subset_custom(['Atheism'])\n",
    "\n",
    "#Start session\n",
    "compute_class = compute_class_weight('balanced', np.unique(true_labels), true_labels)\n",
    "weights = {0: compute_class[0], 1:compute_class[1], 2:compute_class[2]}\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model_seq_custom(max_length = max_length, \n",
    "                     train_layers=6, \n",
    "                     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                                         beta_1 = 0.9,\n",
    "                                                         beta_2 = 0.999),\n",
    "                     dropout_rate= 0.5)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    validation_data = [val_data, val_y],\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    batch_size=32,\n",
    "    class_weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(test_data)\n",
    "metrics(true_labels, test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
