---
title: "Stance Detection in Tweets"
subtitle: "W266 Natural Language Processing - Final Project"
author: "Alex Dessouky & Tim Spittle"
date: "December 7, 2019"
tags: [NLP, BERT, Deep Learning]
abstract: [TBD - Abstract goes here]
output:
  pdf_document:
    fig_caption: no
    number_sections: yes
    toc: no
  md_document:
    variant: markdown
  html_document:
    df_print: paged
    toc: no
bibliography: bibliography.bib
---

```{r packages, include = FALSE}
library(tidyverse)
library(gridExtra)
library(htmltools)
```

# Introduction

Text  

## Background  

More text   

## Objectives  

More text   

# Related Work  

# Data, Task, & Evaluation  

SemEval 2016 Task 6 [see @mohammad-etal-2016-semeval]

```{r confusion_matrix, echo = FALSE, fig.height = 8, fig.width = 8, fig.align="center", fig.cap="\\label{fig:confusion_matrix}Confusion Matrix"}

label_levels = c("Against", "None", "Favor")
topic_name = "abort"
import_topic = function(topic_name){
  tweet_data = read.csv(file = paste0("./final_outputs/", topic_name, "_tweets_v3.csv"), header = FALSE) %>%
  # Check on why tweets have different index? probably maintain original whereas others are built from scratch
    mutate(V1 = 1:n()-1)
  pred_data = read.csv(file = paste0("./final_outputs/", topic_name, "_preds_v3.csv"), header = FALSE)
  true_data = read.csv(file = paste0("./final_outputs/", topic_name, "_true_v3.csv"), header = FALSE)
  names(pred_data) = c("obs_num", "predicted_label")
  names(true_data) = c("obs_num", "true_label")
  names(tweet_data) = c("obs_num", "tweet")
  
  merged = tweet_data %>%
    merge(pred_data
          , by = "obs_num"
          , all.x = TRUE) %>%
    merge(true_data
          , by = "obs_num"
          , all.x = TRUE) %>%
    mutate(topic = topic_name
           , predicted_label_f = case_when(predicted_label == 0 ~ "Against"
                                         , predicted_label == 1 ~ "None"
                                         , predicted_label == 2 ~ "Favor"
                                         , TRUE ~ NA_character_) %>% factor(levels = label_levels)
           , true_label_f = case_when(true_label == 0 ~ "Against"
                                    , true_label == 1 ~ "None"
                                    , true_label == 2 ~ "Favor"
                                    , TRUE ~ NA_character_) %>% factor(levels = label_levels)
    )
  return(merged)
}

abort = import_topic(topic_name = "abort")
atheism = import_topic(topic_name = "atheism")
clim = import_topic(topic_name = "clim")
hil = import_topic(topic_name = "hil")
fem = import_topic(topic_name = "fem")
all_topics = bind_rows(abort, atheism, clim, hil, fem)

metrics = function(y_true, y_pred){
  confusion_matrix = as.matrix(table(Actual = y_true, Predicted = y_pred))
  n = sum(confusion_matrix) 
  n_classes = nrow(confusion_matrix) 
  correct_byclass = diag(confusion_matrix)  
  instances_byclass = apply(confusion_matrix, 1, sum) 
  predictions_byclass = apply(confusion_matrix, 2, sum) 
  
  precision = correct_byclass / predictions_byclass 
  recall = correct_byclass / instances_byclass 
  f1 = 2 * precision * recall / (precision + recall) 

  classification_report = data.frame("Class" = label_levels
                                    , "Precision" = round(precision * 100, 2)
                                    , "Recall" = round(recall * 100, 2)
                                    , "F1 Score" = round(f1 * 100, 2))
  f1_macro = mean(f1)
  f1_macro_custom = mean(f1[c(1,3)])
  
  metric_list = list()
  metric_list$confusion_matrix = confusion_matrix
  metric_list$classification_report = classification_report
  metric_list$f1_macro = f1_macro
  metric_list$f1_macro_custom = f1_macro_custom
  
  return(metric_list)
}

abort_metrics = metrics(y_true = abort$true_label_f, y_pred = abort$predicted_label_f)
atheism_metrics = metrics(y_true = atheism$true_label_f, y_pred = atheism$predicted_label_f)
clim_metrics = metrics(y_true = clim$true_label_f, y_pred = clim$predicted_label_f)
hil_metrics = metrics(y_true = hil$true_label_f, y_pred = hil$predicted_label_f)
fem_metrics = metrics(y_true = fem$true_label_f, y_pred = fem$predicted_label_f)
all_topics_metrics = metrics(y_true = all_topics$true_label_f, y_pred = all_topics$predicted_label_f)

# F stats
f_topics = c(atheism_metrics$f1_macro_custom
             , clim_metrics$f1_macro_custom
             , fem_metrics$f1_macro_custom
             , hil_metrics$f1_macro_custom
             , abort_metrics$f1_macro_custom)
f_summary = c(all_topics_metrics$f1_macro_custom
              , mean(f_topics)
              , f_topics)
f_summary = round(f_summary * 100, 2)
names(f_summary) = c("F-micro", "F-macro"
                     , "Atheism", "Climate Change", "Feminism", "Hillary Clinton", "Abortion")

# Confusion Plot
confusion_plot = ggplot(data =  all_topics %>% group_by(true_label_f, predicted_label_f) %>% summarise(n = n())
       , mapping = aes(x = true_label_f, y = predicted_label_f)) +
  geom_tile(aes(fill = n), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", n)), vjust = 1) +
  scale_fill_gradient(low = "white", high = "green4") +
  theme_bw() + theme(legend.position = "none") +
  labs(x = "True Label", y = "Predicted Label") + 
  scale_y_discrete(limits = rev(label_levels)) +
  scale_x_discrete(limits = rev(label_levels)) +
  labs(title = "All Topics")

grid.arrange(confusion_plot, tableGrob(all_topics_metrics$classification_report %>% select(-Class))
             , tableGrob(data.frame("F1 Score" = f_summary))
             , nrow = 2)
```

\pagebreak

# Methodology  

## Other Work  

BERT [see _\autoref{fig:bert_by_task_fig}_ from @devlin2018bert]

```{r bert_by_task_fig, echo = FALSE, fig.height = 4, fig.width = 4, fig.align="center", fig.cap="\\label{fig:bert_by_task}BERT by Task (Source: @devlin2018bert)"}
knitr::include_graphics(path = "images/bert_bytask.png")
```

\pagebreak

# Model

## Infrastructure

## Hyperparameter Tuning

## Training

| Hyperparameter | Final Value |
| :------------- | :---------- | 
| Learning Rate | 0.001 |
| Dropout Rate | 50% |
| Batch Size | 32 |
| # of Bert Fine Tune Layers | 6 layers |
| Tweet Pre-processing | None |


# Results  

```{r f_micro_macro, echo = FALSE}

measure_levels = c("F-micro", "F-macro", "Atheism", "Climate", "Feminism", "Hillary", "Abortion")

f_data = data.frame("measure" = c("F-micro", "F-macro", "Atheism", "Climate", "Feminism", "Hillary", "Abortion") %>% 
                      factor(levels = measure_levels)
                    , "our_model" = f_summary
                    , "majority_classifer" = c(65.22,	40.092,	42.11,	42.12,	39.1,	36.83,	40.3)
                    ) %>% 
  gather(key = "model", value = "value", -measure) %>%
  mutate(custom_color = c(rep(1, 2), rep(3, 5)
                          , rep(2, 2), rep(4, 5)) %>% factor()) %>%
  arrange(custom_color)

ggplot(data = f_data) + 
  geom_bar(aes(x = measure, y = value, fill = custom_color, group = model), colour = "black", stat = "identity", position = "dodge") +
  scale_fill_manual(name = "Model", labels = c("Our Model - Aggregate", "Majority Classifier - Aggregate"
                                               , "Our Model - by Topic", "Majority Classifier - by Topic"
                                               )
                    , values = c("blue", "red", "cornflowerblue", "coral")) +
  labs(x = "Topic/Measure", y = "Score")
# TO DO - color aggregate F scores differently 
```

| Topic | F1 Score | SemEval-2016 Rank | Overall Rank |
| :--------- | :------: | :---: | :---: |
| Overall Macro F1 | 60.88 | 1 | 2 |
| Overall Micro F1 | 69.72 | 1 | 3 |
| Atheism | 69.66 | 1 | 2 |
| Hillary Clinton | 64.48 | 2 | 3 |
| Abortion | 64.06 | 1 | 3 |
| Climate Change | 48.47 | 3 | 5 |
| Feminism | 57.76 | 2 | 3 |



| Team | Overall (F-micro) | Overall (F-macro) | Atheism | Climate | Feminism | Hillary | Abortion |
| :--------------- | :-----: | :------: | :---: | :---: | :---: | :---: | :---: |
| Muhammed et. al | 70.32 | 59.01 | 69.19 | 43.80 | 58.72 | 61.74 | 66.91 |
| Sun | 69.79 | 61.00 | 70.53 | 49.56 | 57.50 | 61.84 | 66.16 |
| Dessouky & Spittle | 69.72 | 60.88 | 69.66 | 48.47 | 57.76 | 64.48 | 64.06 |
| Du | 68.79 | 59.56 | 59.77 | 53.59 | 55.77 | 65.38 | 63.72 |
| MITRE | 67.82 | 56.03 | 61.47 | 41.63 | 62.09 | 57.67 | 57.28 |
| pkudblab | 67.33 | 58.57 | 63.34 | 52.69 | 51.33 | 64.41 | 61.09 |
| TakeLab | 66.83 | 58.00 | 67.25 | 41.25 | 53.01 | 67.12 | 61.38 |
| Majority Class | 65.22 | 40.09 | 42.11 | 42.12 | 39.10 | 36.83 | 40.30 |
| DeepStance | 63.54 | 52.86 | 52.90 | 40.40 | 52.34 | 55.35 | 63.32 |
| IDI@NTNU | 62.47 | 55.08 | 59.59 | 54.86 | 48.59 | 57.89 | 54.47 |


# Conclusion  

## Discussion  

# Limitations  

## Error Analysis  

```{r error_analysis}

all_topics_errors = all_topics %>%
  filter(predicted_label != true_label) %>%
  select(obs_num, topic, tweet, predicted_label_f, true_label_f)

write.csv(all_topics_errors, file = "./final_outputs/all_topics_errors.csv")

### Abortion
## Potential negation? 
# #pregnancyforall seems like a pro-choice tag but the "I have a right/power of choice" rhetoric is pro-choice
print(all_topics_errors %>% filter(obs_num == 6 & topic == "abort") %>% select(tweet) %>% as.character())
# There is no way to tall from the existing tweet
print(all_topics_errors %>% filter(obs_num == 10 & topic == "abort") %>% select(tweet) %>% as.character())

## Just wrong true label
# There is really no ambiguity here - this person is pro-abortion, should be FAVOR, labeled as AGAINST
print(all_topics_errors %>% filter(obs_num == 13 & topic == "abort") %>% select(tweet) %>% as.character())
```

## Further Work

# References  