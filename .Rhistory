geom_bar(aes(x = measure, y = value, fill = custom_color), colour = "black", stat = "identity", position = "dodge") +
scale_fill_manual(name = "Model"
, breaks = c(1, 2)
, labels = c("Dessouky/Spittle", "Majority Classifier")
, values = c(paired_palette[2], paired_palette[4], paired_palette[1], paired_palette[3])) +
labs(x = "Topic/Measure", y = "Score") +
theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)
, legend.position = "bottom")
# Confusion Plot
ggplot(data =  all_topics %>% group_by(true_label_f, predicted_label_f) %>% summarise(n = n())
, mapping = aes(x = true_label_f, y = predicted_label_f)) +
geom_tile(aes(fill = n), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", n)), vjust = 1) +
scale_fill_gradient(low = "white", high = "green4") +
theme_bw() + theme(legend.position = "none") +
labs(x = "True Label", y = "Predicted Label") +
scale_y_discrete(limits = rev(label_levels)) +
scale_x_discrete(limits = rev(label_levels)) +
labs(caption = "Note: All Topics")
all_topics_errors = all_topics %>%
filter(predicted_label != true_label) %>%
select(obs_num, topic, tweet, predicted_label_f, true_label_f)
##################### Use below if needed to export raw errors again #####################
# write.csv(all_topics_errors, file = "./final_outputs/all_topics_errors.csv")
##########################################################################################
all_topics_errors_reviewed = read.xlsx("./final_outputs/all_topics_errors_reviewed.xlsx", sheet = "all_topics_errors")
atheism_errors_annotated = all_topics_errors_reviewed %>%
filter(topic == "atheism")
atheism_errors_grouped = atheism_errors_annotated %>%
group_by(true_label_f, issue) %>%
summarise(count = n())
ggplot(data = atheism_errors_grouped
, aes(x = true_label_f, y = count, fill = issue)) +
geom_bar(position = "stack", stat = "identity", color = "black") +
labs(x = "True Label", y = "Count") +
scale_fill_discrete(name = "Issue") +
theme_bw() + theme(legend.position = "bottom")
class_report
bert_img = readPNG("images/Dessouky_Spittle_Model_Diagram.PNG")
grid.raster(bert_img)
bert_img = readPNG("images/Dessouky_Spittle_Model_Diagram.PNG")
grid.raster(bert_img)
?grid.raster
all_topics_errors = all_topics %>%
filter(predicted_label != true_label) %>%
select(obs_num, topic, tweet, predicted_label_f, true_label_f)
##################### Use below if needed to export raw errors again #####################
# write.csv(all_topics_errors, file = "./final_outputs/all_topics_errors.csv")
##########################################################################################
all_topics_errors_reviewed = read.xlsx("./final_outputs/all_topics_errors_reviewed.xlsx", sheet = "all_topics_errors")
atheism_errors_annotated = all_topics_errors_reviewed %>%
filter(topic == "atheism")
atheism_errors_grouped = atheism_errors_annotated %>%
group_by(true_label_f, issue) %>%
summarise(count = n())
ggplot(data = atheism_errors_grouped
, aes(x = true_label_f, y = count, fill = issue)) +
geom_bar(position = "stack", stat = "identity", color = "black") +
labs(x = "True Label", y = "Count") +
scale_fill_discrete(name = "Issue") +
theme_bw() + theme(legend.position = "bottom")
atheism_errors_annotated %>% filter(topic == "atheism" & examples == "blatant") %>% select(tweet) %>% gsub("@", "[AT]")
atheism_errors_annotated %>% filter(topic == "atheism" & examples == "blatant") %>% select(tweet)
atheism_errors_annotated %>% filter(topic == "atheism" & examples == "blatant") %>% select(tweet) %>% str_replace("@", "[AT]")
library(tidyverse)
library(gridExtra)
library(htmltools)
library(ggthemes)
library(RColorBrewer)
library(png)
library(grid)
library(openxlsx)
options(tinytex.verbose = TRUE)
label_levels = c("Against", "None", "Favor")
# IMPORT
import_topic = function(topic_name){
tweet_data = read.csv(file = paste0("./final_outputs/", topic_name, "_tweets_v3.csv"), header = FALSE) %>%
# Check on why tweets have different index? probably maintain original whereas others are built from scratch
mutate(V1 = 1:n()-1)
pred_data = read.csv(file = paste0("./final_outputs/", topic_name, "_preds_v3.csv"), header = FALSE)
true_data = read.csv(file = paste0("./final_outputs/", topic_name, "_true_v3.csv"), header = FALSE)
names(pred_data) = c("obs_num", "predicted_label")
names(true_data) = c("obs_num", "true_label")
names(tweet_data) = c("obs_num", "tweet")
merged = tweet_data %>%
merge(pred_data
, by = "obs_num"
, all.x = TRUE) %>%
merge(true_data
, by = "obs_num"
, all.x = TRUE) %>%
mutate(topic = topic_name
, predicted_label_f = case_when(predicted_label == 0 ~ "Against"
, predicted_label == 1 ~ "None"
, predicted_label == 2 ~ "Favor"
, TRUE ~ NA_character_) %>% factor(levels = label_levels)
, true_label_f = case_when(true_label == 0 ~ "Against"
, true_label == 1 ~ "None"
, true_label == 2 ~ "Favor"
, TRUE ~ NA_character_) %>% factor(levels = label_levels)
)
return(merged)
}
abort = import_topic(topic_name = "abort")
atheism = import_topic(topic_name = "atheism")
clim = import_topic(topic_name = "clim")
hil = import_topic(topic_name = "hil")
fem = import_topic(topic_name = "fem")
all_topics = bind_rows(abort, atheism, clim, hil, fem)
# CALCULATE METRICS
metrics = function(y_true, y_pred){
confusion_matrix = as.matrix(table(Actual = y_true, Predicted = y_pred))
n = sum(confusion_matrix)
n_classes = nrow(confusion_matrix)
correct_byclass = diag(confusion_matrix)
instances_byclass = apply(confusion_matrix, 1, sum)
predictions_byclass = apply(confusion_matrix, 2, sum)
precision = correct_byclass / predictions_byclass
recall = correct_byclass / instances_byclass
f1 = 2 * precision * recall / (precision + recall)
classification_report = data.frame("Class" = label_levels
, "Precision" = round(precision * 100, 2)
, "Recall" = round(recall * 100, 2)
, "F1 Score" = round(f1 * 100, 2))
f1_macro = mean(f1)
f1_macro_custom = mean(f1[c(1,3)])
metric_list = list()
metric_list$confusion_matrix = confusion_matrix
metric_list$classification_report = classification_report
metric_list$f1_macro = f1_macro
metric_list$f1_macro_custom = f1_macro_custom
return(metric_list)
}
abort_metrics = metrics(y_true = abort$true_label_f, y_pred = abort$predicted_label_f)
atheism_metrics = metrics(y_true = atheism$true_label_f, y_pred = atheism$predicted_label_f)
clim_metrics = metrics(y_true = clim$true_label_f, y_pred = clim$predicted_label_f)
hil_metrics = metrics(y_true = hil$true_label_f, y_pred = hil$predicted_label_f)
fem_metrics = metrics(y_true = fem$true_label_f, y_pred = fem$predicted_label_f)
all_topics_metrics = metrics(y_true = all_topics$true_label_f, y_pred = all_topics$predicted_label_f)
# F stats
f_topics = c(atheism_metrics$f1_macro_custom
, clim_metrics$f1_macro_custom
, fem_metrics$f1_macro_custom
, hil_metrics$f1_macro_custom
, abort_metrics$f1_macro_custom)
f_summary = c(all_topics_metrics$f1_macro_custom
, mean(f_topics)
, f_topics)
f_summary = round(f_summary * 100, 2)
names(f_summary) = c("F-micro", "F-macro"
, "Atheism", "Climate Change", "Feminism", "Hillary Clinton", "Abortion")
bert_img = readPNG("images/bert_bytask.png")
grid.raster(bert_img)
bert_img = readPNG("images/Dessouky_Spittle_Model_Diagram.PNG")
grid.raster(bert_img)
measure_levels = c("F-micro", "F-macro", "Atheism", "Climate", "Feminism", "Hillary", "Abortion")
f_data = data.frame("measure" = c("F-microT", "F-macroT", "Atheism", "Climate", "Feminism", "Hillary", "Abortion") %>%
factor(levels = measure_levels)
, "our_model" = f_summary
, "majority_classifer" = c(65.22,	40.092,	42.11,	42.12,	39.1,	36.83,	40.3)
) %>%
gather(key = "model", value = "value", -measure) %>%
mutate(custom_color = c(rep(1, 2), rep(3, 5)
, rep(2, 2), rep(4, 5)) %>% factor()) %>%
arrange(custom_color)
paired_palette = brewer.pal(n = 4, name = "Paired")
ggplot(data = f_data) +
geom_bar(aes(x = measure, y = value, fill = custom_color), colour = "black", stat = "identity", position = "dodge") +
scale_fill_manual(name = "Model"
, breaks = c(1, 2)
, labels = c("Dessouky/Spittle", "Majority Classifier")
, values = c(paired_palette[2], paired_palette[4], paired_palette[1], paired_palette[3])) +
labs(x = "Topic/Measure", y = "Score") +
theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)
, legend.position = "bottom")
# Confusion Plot
ggplot(data =  all_topics %>% group_by(true_label_f, predicted_label_f) %>% summarise(n = n())
, mapping = aes(x = true_label_f, y = predicted_label_f)) +
geom_tile(aes(fill = n), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", n)), vjust = 1) +
scale_fill_gradient(low = "white", high = "green4") +
theme_bw() + theme(legend.position = "none") +
labs(x = "True Label", y = "Predicted Label") +
scale_y_discrete(limits = rev(label_levels)) +
scale_x_discrete(limits = rev(label_levels)) +
labs(caption = "Note: All Topics")
all_topics_errors = all_topics %>%
filter(predicted_label != true_label) %>%
select(obs_num, topic, tweet, predicted_label_f, true_label_f)
##################### Use below if needed to export raw errors again #####################
# write.csv(all_topics_errors, file = "./final_outputs/all_topics_errors.csv")
##########################################################################################
all_topics_errors_reviewed = read.xlsx("./final_outputs/all_topics_errors_reviewed.xlsx", sheet = "all_topics_errors")
atheism_errors_annotated = all_topics_errors_reviewed %>%
filter(topic == "atheism")
atheism_errors_grouped = atheism_errors_annotated %>%
group_by(true_label_f, issue) %>%
summarise(count = n())
ggplot(data = atheism_errors_grouped
, aes(x = true_label_f, y = count, fill = issue)) +
geom_bar(position = "stack", stat = "identity", color = "black") +
labs(x = "True Label", y = "Count") +
scale_fill_discrete(name = "Issue") +
theme_bw() + theme(legend.position = "bottom")
atheism_errors_annotated %>% filter(topic == "atheism" & examples == "sarcasm" & issue == "Our modell wrong") %>% select(tweet) %>% str_replace("@", "[AT]")
atheism_errors_annotated %>% filter(topic == "atheism" & examples == "sarcasm" & issue == "Our modell wrong") %>% select(tweet)
atheism_errors_annotated %>% filter(topic == "atheism" & examples == "sarcasm" & issue == "Our modell wrong")
atheism_errors_annotated %>% filter(topic == "atheism" & examples == "sarcasm")
library(tidyverse)
library(gridExtra)
library(htmltools)
library(ggthemes)
library(RColorBrewer)
library(png)
library(grid)
library(openxlsx)
options(tinytex.verbose = TRUE)
label_levels = c("Against", "None", "Favor")
# IMPORT
import_topic = function(topic_name){
tweet_data = read.csv(file = paste0("./final_outputs/", topic_name, "_tweets_v3.csv"), header = FALSE) %>%
# Check on why tweets have different index? probably maintain original whereas others are built from scratch
mutate(V1 = 1:n()-1)
pred_data = read.csv(file = paste0("./final_outputs/", topic_name, "_preds_v3.csv"), header = FALSE)
true_data = read.csv(file = paste0("./final_outputs/", topic_name, "_true_v3.csv"), header = FALSE)
names(pred_data) = c("obs_num", "predicted_label")
names(true_data) = c("obs_num", "true_label")
names(tweet_data) = c("obs_num", "tweet")
merged = tweet_data %>%
merge(pred_data
, by = "obs_num"
, all.x = TRUE) %>%
merge(true_data
, by = "obs_num"
, all.x = TRUE) %>%
mutate(topic = topic_name
, predicted_label_f = case_when(predicted_label == 0 ~ "Against"
, predicted_label == 1 ~ "None"
, predicted_label == 2 ~ "Favor"
, TRUE ~ NA_character_) %>% factor(levels = label_levels)
, true_label_f = case_when(true_label == 0 ~ "Against"
, true_label == 1 ~ "None"
, true_label == 2 ~ "Favor"
, TRUE ~ NA_character_) %>% factor(levels = label_levels)
)
return(merged)
}
abort = import_topic(topic_name = "abort")
atheism = import_topic(topic_name = "atheism")
clim = import_topic(topic_name = "clim")
hil = import_topic(topic_name = "hil")
fem = import_topic(topic_name = "fem")
all_topics = bind_rows(abort, atheism, clim, hil, fem)
# CALCULATE METRICS
metrics = function(y_true, y_pred){
confusion_matrix = as.matrix(table(Actual = y_true, Predicted = y_pred))
n = sum(confusion_matrix)
n_classes = nrow(confusion_matrix)
correct_byclass = diag(confusion_matrix)
instances_byclass = apply(confusion_matrix, 1, sum)
predictions_byclass = apply(confusion_matrix, 2, sum)
precision = correct_byclass / predictions_byclass
recall = correct_byclass / instances_byclass
f1 = 2 * precision * recall / (precision + recall)
classification_report = data.frame("Class" = label_levels
, "Precision" = round(precision * 100, 2)
, "Recall" = round(recall * 100, 2)
, "F1 Score" = round(f1 * 100, 2))
f1_macro = mean(f1)
f1_macro_custom = mean(f1[c(1,3)])
metric_list = list()
metric_list$confusion_matrix = confusion_matrix
metric_list$classification_report = classification_report
metric_list$f1_macro = f1_macro
metric_list$f1_macro_custom = f1_macro_custom
return(metric_list)
}
abort_metrics = metrics(y_true = abort$true_label_f, y_pred = abort$predicted_label_f)
atheism_metrics = metrics(y_true = atheism$true_label_f, y_pred = atheism$predicted_label_f)
clim_metrics = metrics(y_true = clim$true_label_f, y_pred = clim$predicted_label_f)
hil_metrics = metrics(y_true = hil$true_label_f, y_pred = hil$predicted_label_f)
fem_metrics = metrics(y_true = fem$true_label_f, y_pred = fem$predicted_label_f)
all_topics_metrics = metrics(y_true = all_topics$true_label_f, y_pred = all_topics$predicted_label_f)
# F stats
f_topics = c(atheism_metrics$f1_macro_custom
, clim_metrics$f1_macro_custom
, fem_metrics$f1_macro_custom
, hil_metrics$f1_macro_custom
, abort_metrics$f1_macro_custom)
f_summary = c(all_topics_metrics$f1_macro_custom
, mean(f_topics)
, f_topics)
f_summary = round(f_summary * 100, 2)
names(f_summary) = c("F-micro", "F-macro"
, "Atheism", "Climate Change", "Feminism", "Hillary Clinton", "Abortion")
bert_img = readPNG("images/bert_bytask.png")
grid.raster(bert_img)
knitr::include_graphics("images/Dessouky_Spittle_Model_Diagram.PNG")
measure_levels = c("F-micro", "F-macro", "Atheism", "Climate", "Feminism", "Hillary", "Abortion")
f_data = data.frame("measure" = c("F-microT", "F-macroT", "Atheism", "Climate", "Feminism", "Hillary", "Abortion") %>%
factor(levels = measure_levels)
, "our_model" = f_summary
, "majority_classifer" = c(65.22,	40.092,	42.11,	42.12,	39.1,	36.83,	40.3)
) %>%
gather(key = "model", value = "value", -measure) %>%
mutate(custom_color = c(rep(1, 2), rep(3, 5)
, rep(2, 2), rep(4, 5)) %>% factor()) %>%
arrange(custom_color)
paired_palette = brewer.pal(n = 4, name = "Paired")
ggplot(data = f_data) +
geom_bar(aes(x = measure, y = value, fill = custom_color), colour = "black", stat = "identity", position = "dodge") +
scale_fill_manual(name = "Model"
, breaks = c(1, 2)
, labels = c("Dessouky/Spittle", "Majority Classifier")
, values = c(paired_palette[2], paired_palette[4], paired_palette[1], paired_palette[3])) +
labs(x = "Topic/Measure", y = "Score") +
theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)
, legend.position = "bottom")
# Confusion Plot
ggplot(data =  all_topics %>% group_by(true_label_f, predicted_label_f) %>% summarise(n = n())
, mapping = aes(x = true_label_f, y = predicted_label_f)) +
geom_tile(aes(fill = n), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", n)), vjust = 1) +
scale_fill_gradient(low = "white", high = "green4") +
theme_bw() + theme(legend.position = "none") +
labs(x = "True Label", y = "Predicted Label") +
scale_y_discrete(limits = rev(label_levels)) +
scale_x_discrete(limits = rev(label_levels)) +
labs(caption = "Note: All Topics")
all_topics_errors = all_topics %>%
filter(predicted_label != true_label) %>%
select(obs_num, topic, tweet, predicted_label_f, true_label_f)
##################### Use below if needed to export raw errors again #####################
# write.csv(all_topics_errors, file = "./final_outputs/all_topics_errors.csv")
##########################################################################################
all_topics_errors_reviewed = read.xlsx("./final_outputs/all_topics_errors_reviewed.xlsx", sheet = "all_topics_errors")
atheism_errors_annotated = all_topics_errors_reviewed %>%
filter(topic == "atheism")
atheism_errors_grouped = atheism_errors_annotated %>%
group_by(true_label_f, issue) %>%
summarise(count = n())
ggplot(data = atheism_errors_grouped
, aes(x = true_label_f, y = count, fill = issue)) +
geom_bar(position = "stack", stat = "identity", color = "black") +
labs(x = "True Label", y = "Count") +
scale_fill_discrete(name = "Issue") +
theme_bw() + theme(legend.position = "bottom")
f_data
measure_levels = c("F-microT", "F-macroT", "Atheism", "Climate", "Feminism", "Hillary", "Abortion")
f_data = data.frame("measure" = c("F-microT", "F-macroT", "Atheism", "Climate", "Feminism", "Hillary", "Abortion") %>%
factor(levels = measure_levels)
, "our_model" = f_summary
, "majority_classifer" = c(65.22,	40.092,	42.11,	42.12,	39.1,	36.83,	40.3)
) %>%
gather(key = "model", value = "value", -measure) %>%
mutate(custom_color = c(rep(1, 2), rep(3, 5)
, rep(2, 2), rep(4, 5)) %>% factor()) %>%
arrange(custom_color)
paired_palette = brewer.pal(n = 4, name = "Paired")
ggplot(data = f_data) +
geom_bar(aes(x = measure, y = value, fill = custom_color), colour = "black", stat = "identity", position = "dodge") +
scale_fill_manual(name = "Model"
, breaks = c(1, 2)
, labels = c("Dessouky/Spittle", "Majority Classifier")
, values = c(paired_palette[2], paired_palette[4], paired_palette[1], paired_palette[3])) +
labs(x = "Topic/Measure", y = "Score") +
theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)
, legend.position = "bottom")
bert_img = readPNG("images/bert_bytask.png")
grid.raster(bert_img)
library(tidyverse)
library(gridExtra)
library(htmltools)
library(ggthemes)
library(RColorBrewer)
library(png)
library(grid)
library(openxlsx)
options(tinytex.verbose = TRUE)
label_levels = c("Against", "None", "Favor")
# IMPORT
import_topic = function(topic_name){
tweet_data = read.csv(file = paste0("./final_outputs/", topic_name, "_tweets_v3.csv"), header = FALSE) %>%
# Check on why tweets have different index? probably maintain original whereas others are built from scratch
mutate(V1 = 1:n()-1)
pred_data = read.csv(file = paste0("./final_outputs/", topic_name, "_preds_v3.csv"), header = FALSE)
true_data = read.csv(file = paste0("./final_outputs/", topic_name, "_true_v3.csv"), header = FALSE)
names(pred_data) = c("obs_num", "predicted_label")
names(true_data) = c("obs_num", "true_label")
names(tweet_data) = c("obs_num", "tweet")
merged = tweet_data %>%
merge(pred_data
, by = "obs_num"
, all.x = TRUE) %>%
merge(true_data
, by = "obs_num"
, all.x = TRUE) %>%
mutate(topic = topic_name
, predicted_label_f = case_when(predicted_label == 0 ~ "Against"
, predicted_label == 1 ~ "None"
, predicted_label == 2 ~ "Favor"
, TRUE ~ NA_character_) %>% factor(levels = label_levels)
, true_label_f = case_when(true_label == 0 ~ "Against"
, true_label == 1 ~ "None"
, true_label == 2 ~ "Favor"
, TRUE ~ NA_character_) %>% factor(levels = label_levels)
)
return(merged)
}
abort = import_topic(topic_name = "abort")
atheism = import_topic(topic_name = "atheism")
clim = import_topic(topic_name = "clim")
hil = import_topic(topic_name = "hil")
fem = import_topic(topic_name = "fem")
all_topics = bind_rows(abort, atheism, clim, hil, fem)
# CALCULATE METRICS
metrics = function(y_true, y_pred){
confusion_matrix = as.matrix(table(Actual = y_true, Predicted = y_pred))
n = sum(confusion_matrix)
n_classes = nrow(confusion_matrix)
correct_byclass = diag(confusion_matrix)
instances_byclass = apply(confusion_matrix, 1, sum)
predictions_byclass = apply(confusion_matrix, 2, sum)
precision = correct_byclass / predictions_byclass
recall = correct_byclass / instances_byclass
f1 = 2 * precision * recall / (precision + recall)
classification_report = data.frame("Class" = label_levels
, "Precision" = round(precision * 100, 2)
, "Recall" = round(recall * 100, 2)
, "F1 Score" = round(f1 * 100, 2))
f1_macro = mean(f1)
f1_macro_custom = mean(f1[c(1,3)])
metric_list = list()
metric_list$confusion_matrix = confusion_matrix
metric_list$classification_report = classification_report
metric_list$f1_macro = f1_macro
metric_list$f1_macro_custom = f1_macro_custom
return(metric_list)
}
abort_metrics = metrics(y_true = abort$true_label_f, y_pred = abort$predicted_label_f)
atheism_metrics = metrics(y_true = atheism$true_label_f, y_pred = atheism$predicted_label_f)
clim_metrics = metrics(y_true = clim$true_label_f, y_pred = clim$predicted_label_f)
hil_metrics = metrics(y_true = hil$true_label_f, y_pred = hil$predicted_label_f)
fem_metrics = metrics(y_true = fem$true_label_f, y_pred = fem$predicted_label_f)
all_topics_metrics = metrics(y_true = all_topics$true_label_f, y_pred = all_topics$predicted_label_f)
# F stats
f_topics = c(atheism_metrics$f1_macro_custom
, clim_metrics$f1_macro_custom
, fem_metrics$f1_macro_custom
, hil_metrics$f1_macro_custom
, abort_metrics$f1_macro_custom)
f_summary = c(all_topics_metrics$f1_macro_custom
, mean(f_topics)
, f_topics)
f_summary = round(f_summary * 100, 2)
names(f_summary) = c("F-micro", "F-macro"
, "Atheism", "Climate Change", "Feminism", "Hillary Clinton", "Abortion")
# Error Analysis
all_topics_errors = all_topics %>%
filter(predicted_label != true_label) %>%
select(obs_num, topic, tweet, predicted_label_f, true_label_f)
##################### Use below if needed to export raw errors again #####################
# write.csv(all_topics_errors, file = "./final_outputs/all_topics_errors.csv")
##########################################################################################
all_topics_errors_reviewed = read.xlsx("./final_outputs/all_topics_errors_reviewed.xlsx", sheet = "all_topics_errors")
bert_img = readPNG("images/bert_bytask.png")
grid.raster(bert_img)
# knitr::include_graphics("images/Dessouky_Spittle_Model_Diagram.PNG")
bert_img = readPNG("images/Dessouky_Spittle_Model_Diagram.PNG")
grid.raster(bert_img)
measure_levels = c("F-microT", "F-macroT", "Atheism", "Climate", "Feminism", "Hillary", "Abortion")
f_data = data.frame("measure" = c("F-microT", "F-macroT", "Atheism", "Climate", "Feminism", "Hillary", "Abortion") %>%
factor(levels = measure_levels)
, "our_model" = f_summary
, "majority_classifer" = c(65.22,	40.092,	42.11,	42.12,	39.1,	36.83,	40.3)
) %>%
gather(key = "model", value = "value", -measure) %>%
mutate(custom_color = c(rep(1, 2), rep(3, 5)
, rep(2, 2), rep(4, 5)) %>% factor()) %>%
arrange(custom_color)
paired_palette = brewer.pal(n = 4, name = "Paired")
ggplot(data = f_data) +
geom_bar(aes(x = measure, y = value, fill = custom_color), colour = "black", stat = "identity", position = "dodge") +
scale_fill_manual(name = "Model"
, breaks = c(1, 2)
, labels = c("Dessouky/Spittle", "Majority Classifier")
, values = c(paired_palette[2], paired_palette[4], paired_palette[1], paired_palette[3])) +
labs(x = "Topic/Measure", y = "Score") +
theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)
, legend.position = "bottom")
# Confusion Plot
ggplot(data =  all_topics %>% group_by(true_label_f, predicted_label_f) %>% summarise(n = n())
, mapping = aes(x = true_label_f, y = predicted_label_f)) +
geom_tile(aes(fill = n), colour = "white") +
geom_text(aes(label = sprintf("%1.0f", n)), vjust = 1) +
scale_fill_gradient(low = "white", high = "green4") +
theme_bw() + theme(legend.position = "none") +
labs(x = "True Label", y = "Predicted Label") +
scale_y_discrete(limits = rev(label_levels)) +
scale_x_discrete(limits = rev(label_levels)) +
labs(caption = "Note: All Topics")
atheism_errors_annotated = all_topics_errors_reviewed %>%
filter(topic == "atheism")
atheism_errors_grouped = atheism_errors_annotated %>%
group_by(true_label_f, issue) %>%
summarise(count = n())
ggplot(data = atheism_errors_grouped
, aes(x = true_label_f, y = count, fill = issue)) +
geom_bar(position = "stack", stat = "identity", color = "black") +
labs(x = "True Label", y = "Count") +
scale_fill_discrete(name = "Issue") +
theme_bw() + theme(legend.position = "bottom")
