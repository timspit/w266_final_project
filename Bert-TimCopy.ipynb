{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PATH STUFF FIRST SO EASY TO SWITCH BETWEEN US ##\n",
    "\n",
    "# Set working directory\n",
    "# os.chdir(\"/Users/alexdessouky/Desktop/MIDS/w266\")\n",
    "os.chdir(\"/Users/manat/OneDrive/Documents/Tim/MIDS/266_NLP/Final Project\")\n",
    "\n",
    "# Store the paths to bert & data\n",
    "# bert_path =   '/Users/alexdessouky/Desktop/MIDS/w266/bert' \n",
    "# data_path = '/Users/alexdessouky/Desktop/MIDS/w266/w266_final_project/StanceDataset'  \n",
    "# bert_path =   '/Users/manat/OneDrive/Documents/Tim/MIDS/266_NLP/w266/bert' \n",
    "# data_path = '/Users/manat/OneDrive/Documents/Tim/MIDS/266_NLP/Final Project/w266_final_project/StanceDataset'  \n",
    "local_bert_path = r'C:\\Users\\manat\\OneDrive\\Documents\\Tim\\MIDS\\266_NLP\\w266\\bert\\\\' # change as needed\n",
    "data_path = r'C:\\Users\\manat\\OneDrive\\Documents\\Tim\\MIDS\\266_NLP\\Final Project\\w266_final_project\\StanceDataset'  \n",
    "\n",
    "# Make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0, bert_path)\n",
    "sys.path.insert(0, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and Storage\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# Data structures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Strings\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub\n",
    "# Tensorflow hub path to BERT module of choice\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "# Outputs\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Target</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Opinion Towards</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@tedcruz And, #HandOverTheServer she wiped cle...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>1.  The tweet explicitly expresses opinion abo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hillary is our best choice if we truly want to...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>1.  The tweet explicitly expresses opinion abo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@TheView I think our country is ready for a fe...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>1.  The tweet explicitly expresses opinion abo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just gave an unhealthy amount of my hard-ear...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>1.  The tweet explicitly expresses opinion abo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@PortiaABoulger Thank you for adding me to you...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>NONE</td>\n",
       "      <td>3.  The tweet is not explicitly expressing opi...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet           Target  \\\n",
       "0  @tedcruz And, #HandOverTheServer she wiped cle...  Hillary Clinton   \n",
       "1  Hillary is our best choice if we truly want to...  Hillary Clinton   \n",
       "2  @TheView I think our country is ready for a fe...  Hillary Clinton   \n",
       "3  I just gave an unhealthy amount of my hard-ear...  Hillary Clinton   \n",
       "4  @PortiaABoulger Thank you for adding me to you...  Hillary Clinton   \n",
       "\n",
       "    Stance                                    Opinion Towards Sentiment  \n",
       "0  AGAINST  1.  The tweet explicitly expresses opinion abo...       neg  \n",
       "1    FAVOR  1.  The tweet explicitly expresses opinion abo...       pos  \n",
       "2  AGAINST  1.  The tweet explicitly expresses opinion abo...       neg  \n",
       "3  AGAINST  1.  The tweet explicitly expresses opinion abo...       neg  \n",
       "4     NONE  3.  The tweet is not explicitly expressing opi...       pos  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "# Training data\n",
    "twitter_train_raw = pd.read_excel('./w266_final_project/StanceDataset/train.xlsx')\n",
    "\n",
    "# Test data\n",
    "twitter_test_raw = pd.read_excel('./w266_final_project/StanceDataset/test.xlsx')\n",
    "\n",
    "twitter_train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEMPORARY SOLUTION #### - limit to Hillary upfront\n",
    "twitter_train = input_train['data']['Target'].apply(lambda x: x.lower() in topics_selected)\n",
    "twitter_test = input_test['data']['Target'].apply(lambda x: x.lower() in topics_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Data  \n",
    "\n",
    "### Labels $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AGAINST' 'FAVOR' 'NONE']\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# PREP LABELS FOR NN\n",
    "stance_train = np.array(twitter_train['Stance'])\n",
    "stance_test = np.array(twitter_test['Stance'])\n",
    "\n",
    "# One-hot Encoder\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')\n",
    "enc.fit(stance_train.reshape(-1, 1))\n",
    "label_categories = enc.categories_[0]\n",
    "\n",
    "labels_train = enc.transform(stance_train.reshape(-1, 1)).toarray()\n",
    "labels_test = enc.transform(stance_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(label_categories)\n",
    "print(labels_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: @tedcruz And, #HandOverTheServer she wiped clean + 30k deleted emails, explains dereliction of duty/lies re #Benghazi,etc #tcot\n",
      "Clean: @tedcruz and #handovertheserver she wiped clean  DIGITk deleted emails explains dereliction of dutylies re #benghazietc #tcot\n"
     ]
    }
   ],
   "source": [
    "def preprocess_tweets(x):\n",
    "    \n",
    "    # Remove punctuation EXCEPT for hashtags (#) and handles (@)\n",
    "    exclude_punc = [punc for punc in string.punctuation if punc not in ['#', '@']]\n",
    "    x_nopunc = ''.join(ch for ch in x if ch not in exclude_punc)\n",
    "\n",
    "    # lower case\n",
    "    x_lower = x_nopunc.lower()\n",
    "    \n",
    "    # Replace digits with DIGIT\n",
    "    x_digits = re.sub(\"\\d+\", \"DIGIT\", x_lower)\n",
    "    \n",
    "    return x_digits\n",
    "\n",
    "# Clean tweests\n",
    "twitter_train['tweet_clean'] = np.array(twitter_train['Tweet'].apply(lambda x: preprocess_tweets(x)))\n",
    "twitter_test['tweet_clean'] = np.array(twitter_test['Tweet'].apply(lambda x: preprocess_tweets(x)))\n",
    "\n",
    "# Example tweet\n",
    "print(\"Raw: \" + str(twitter_train['Tweet'][0]))\n",
    "print(\"Clean: \" + str(preprocess_tweets(twitter_train['Tweet'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @tedcruz and #handovertheserver she wiped clea...\n",
       "1    hillary is our best choice if we truly want to...\n",
       "2    @theview i think our country is ready for a fe...\n",
       "3    i just gave an unhealthy amount of my hardearn...\n",
       "4    @portiaaboulger thank you for adding me to you...\n",
       "Name: tweet_clean, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = twitter_train['tweet_clean']\n",
    "test_x = twitter_test['tweet_clean']\n",
    "\n",
    "train_x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "bert_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximal length of input 'sentences' (post tokenization).\n",
    "max_length = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/manat/OneDrive/Documents/Tim/MIDS/266_NLP/w266/bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/manat/OneDrive/Documents/Tim/MIDS/266_NLP/w266/bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokens surrounded by the [CLS] and [SEP] tokens\n",
    "train_tokens = train_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])\n",
    "test_tokens = test_x.apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tedcruz and #handovertheserver she wiped clean  DIGITk deleted emails explains dereliction of dutylies re #benghazietc #tcot\n",
      "['[CLS]', '@', 'te', '##d', '##c', '##ru', '##z', 'and', '#', 'hand', '##over', '##the', '##serve', '##r', 'she', 'wiped', 'clean', 'D', '##IG', '##IT', '##k', 'deleted', 'emails', 'explains', 'der', '##eli', '##ction', 'of', 'duty', '##lies', 're', '#', 'ben', '##gh', '##azi', '##et', '##c', '#', 't', '##cot', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(train_tokens[0])\n",
    "\n",
    "# NOTES: why such small tokens? e.g. benghazi has to be an important token by itself, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask ids (mask out the paddings)\n",
    "train_mask_ids = train_tokens.apply(lambda x: len(x)*[1])\n",
    "test_mask_ids = test_tokens.apply(lambda x: len(x)*[1])\n",
    "\n",
    "train_mask_ids = train_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                      np.array(x)).tolist()\n",
    "test_mask_ids = test_mask_ids.apply(lambda x: np.array(x + (max_length - len(x)) * [0]) if len(x) < max_length else \n",
    "                                    np.array(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_mask_ids[0])\n",
    "\n",
    "# NOTES: unclear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding to tokens\n",
    "train_tokens = train_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)\n",
    "test_tokens = test_tokens.apply(lambda x: x + (max_length - len(x)) * ['[PAD]'] if len(x) < max_length else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '@', 'te', '##d', '##c', '##ru', '##z', 'and', '#', 'hand', '##over', '##the', '##serve', '##r', 'she', 'wiped', 'clean', 'D', '##IG', '##IT', '##k', 'deleted', 'emails', 'explains', 'der', '##eli', '##ction', 'of', 'duty', '##lies', 're', '#', 'ben', '##gh', '##azi', '##et', '##c', '#', 't', '##cot', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[0])\n",
    "\n",
    "# NOTES: every tweet same length of tokens (max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence vectors\n",
    "train_sequenceids = train_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "test_sequenceids = test_tokens.apply(lambda x: np.array(max_length*[0])).tolist()\n",
    "\n",
    "# Convert tokens to sentence ids\n",
    "train_sentenceids = train_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()\n",
    "test_sentenceids = test_tokens.apply(lambda x: np.array(tokenizer.convert_tokens_to_ids(x))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 101 4665 3113 1110 1412 1436 3026 1191 1195 5098 1328 1106 2760 1217\n",
      "  170 8706 3790  108 9294 2660  102    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequenceids[0])\n",
    "print(train_sequenceids[100])\n",
    "print(train_sentenceids[1])\n",
    "\n",
    "# NOTES: what are sequence ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert features\n",
    "bert_train = [np.array(train_sentenceids), np.array(train_mask_ids), np.array(train_sequenceids)]\n",
    "bert_test = [np.array(test_sentenceids), np.array(test_mask_ids), np.array(test_sequenceids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  101,   137, 21359,  1181,  1665,  5082,  1584,  1105,   108,\n",
      "        1289,  5909, 10681, 17886,  1197,  1131,  7960,  4044,   141,\n",
      "       23413, 12150,  1377, 18931, 24853,  7155,  4167, 21091,  5796,\n",
      "        1104,  4019,  7875,  1231,   108, 26181,  5084, 19888,  2105,\n",
      "        1665,   108,   189, 18982,   102,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0])\n",
      " array([ 101, 4665, 3113, 1110, 1412, 1436, 3026, 1191, 1195, 5098, 1328,\n",
      "       1106, 2760, 1217,  170, 8706, 3790,  108, 9294, 2660,  102,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0])\n",
      " array([ 101,  137, 1103, 7334,  178, 1341, 1412, 1583, 1110, 2407, 1111,\n",
      "        170, 2130, 3073, 1116, 1122, 1169, 1204, 1518, 1129, 4665, 3113,\n",
      "        102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0])\n",
      " ...\n",
      " array([  101,  1293,  9164,  1128,  1474,  1139,  3785, 12629,  1110,\n",
      "         170,  3026,  1293,  9164,  1128,  1474,  4267,  6602,  5521,\n",
      "       26368,  1139,  3073,  7107,  2762,  1204,   108,  7691,  1863,\n",
      "         108, 14516,  4206,  1204,   102,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0])\n",
      " array([  101,  4463,  2266,  1111,  1343,  1255,  1115,  1236,  1185,\n",
      "        2266,  1111,  1343,  1136,  1870,  1255,   108,  7691, 13791,\n",
      "        1596,   108,  7691,  1863,   108,  1297,   108, 14516,  4206,\n",
      "        1204,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0])\n",
      " array([  101,   108,  9814,  1361, 19551,  1117, 10261,   192,   141,\n",
      "       23413, 12150,  1202,  1584,  4646,  1103,   108,  1301,  1643,\n",
      "       12932,  1253, 26228,   108,  2560,  3892,   108,   184,  2822,\n",
      "        1918, 23340,  8214,  2469,   141, 23413, 12150,   108,  1703,\n",
      "       20316,  1733,   108,  1185,  3269,  2225,   108, 14516,  4206,\n",
      "        1204,   102,     0,     0])]\n"
     ]
    }
   ],
   "source": [
    "print(bert_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layer to create Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_url,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        \n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "        \n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        \n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        \n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(2914,)\n",
      "(49,)\n",
      "(2914, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(bert_train))\n",
    "print(bert_train[0].shape)\n",
    "print(bert_train[0][0].shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES: \n",
    "    # why have max_input_length as a parameter when we use max_length?\n",
    "\n",
    "def bert_model(max_input_length, train_layers, optimizer = tf.keras.optimizers.Adam(learning_rate=1)):\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_sequence = BertLayer(n_fine_tune_layers = train_layers)(bert_inputs)\n",
    "    \n",
    "    #dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dropout = tf.keras.layers.Dropout(rate=0.3)(bert_sequence)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(3, activation='softmax', name='classification')(dropout)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['categorical_accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        (None, 768)          108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 768)          0           bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 3)            2307        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 108,933,703\n",
      "Trainable params: 6,498,051\n",
      "Non-trainable params: 102,435,652\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_ids to have shape (50,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-1f0b2a3bcd40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     batch_size=32)\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m#callbacks=[tensorboard]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    383\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    386\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_ids to have shape (50,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#Start session\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = bert_model(max_input_length = max_length + 1, \n",
    "                   train_layers = 12, \n",
    "                   optimizer = 'adam')\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    x = bert_train, \n",
    "    y = labels_train,\n",
    "    validation_split = 0.2,\n",
    "    shuffle = True,\n",
    "    epochs = 30,\n",
    "    verbose = 1,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# class_weight = \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(bert_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_plot(confusion_matrix, target_names):\n",
    "    # Plot confusion matrix (via imshow)\n",
    "    plt.imshow(confusion_matrix, interpolation = \"nearest\", cmap = plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Loop through each value of the matrix to add data labels\n",
    "    width, height = confusion_matrix.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(str(confusion_matrix[x][y]), xy = (y, x), \n",
    "                        horizontalalignment = \"center\",\n",
    "                        verticalalignment = \"center\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    \n",
    "def metrics(true_labels, test_probs):\n",
    "    \n",
    "    #find predicted labels\n",
    "    test_predicts = np.argmax(test_probs, axis = 1)\n",
    "    \n",
    "    #calculate f1 score\n",
    "    f1 = f1_score(true_labels, test_predicts, average = 'macro')\n",
    "    \n",
    "    print(\"F1 macro score:\", f1)\n",
    "    \n",
    "    print(classification_report(y_true = true_labels, \n",
    "                                        y_pred = test_predicts,\n",
    "                                        target_names = ['Against', 'None', 'Favor']))\n",
    "    \n",
    "    confuse = confusion_matrix(y_true = true_labels, y_pred = test_predicts)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    confusion_plot(confuse, ['Against', 'None', 'Favor'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
